{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Raw Cell Format","colab":{"provenance":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-u209K9xPtzU"},"source":["![Solvay Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Solvay_Brussels_School_logo.svg/1280px-Solvay_Brussels_School_logo.svg.png)\n","# TP 5 Data Quality\n","## Background"]},{"cell_type":"markdown","metadata":{"id":"BLJAnC5qPtzi"},"source":["You did it! You managed to retrieve information either from the web or from another source. Things are getting pretty serious.\n","\n","Usually, you will see that you have to preprocess the data before using it in your models. This is the goal of the session: making sure we have clean and well-formatted data to work with."]},{"cell_type":"markdown","metadata":{"id":"YshvKL2DPtzs"},"source":["In this session, we will talk about the following issues or challenges arising from real-world data:\n","1. Missing Values\n","2. Outliers\n","3. Duplicates\n","4. Frequent transformation and formatting"]},{"cell_type":"markdown","metadata":{"id":"SrzrE7i4Ptz1"},"source":["# Missing Values"]},{"cell_type":"markdown","metadata":{"id":"lsFU2eZKPtz7"},"source":["One common issue are the missing values.\n","\n","The causes are multiple:\n","* For survey, certain people leave answers blank\n","* Some information is unknown for certain observations (i.e. shares of different religious groups are public in certain countries but not in other)\n","* When reconciling data from different sources, some info may be present in one source and not in other\n","* There may have been mishandling of data\n","* etc."]},{"cell_type":"markdown","metadata":{"id":"PV79fLe4Pt0K"},"source":["There are several ways to get rid of those. But one has to be careful about their impact on the final result and always perform checks to assess the consequences of the chosen method.\n","\n","In the following exercises, we will build an hypothetical example and see what are the consequences of several ways of handling missing values."]},{"cell_type":"markdown","metadata":{"id":"yhXiTn1APt2N"},"source":["In this scenario, we have two independent variables (or features) and one dependent variable (or target). We want to identify the coefficients that enable us to retrieve the dependent variable with basis on the independent ones.\n","\n","Imagine the following scenario: children and dogs both eat biscuits, while parents don't. You want to form an estimate of the money each family is going to spend on biscuit in a given year. You strongly suspect that there is a direct linear correlation between the number of children in a family and the total spent on biscuits as well as the number of dogs a family has and the total spent on biscuits."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:12:22.119881Z","start_time":"2019-03-12T07:12:04.053578Z"},"id":"9u-2K3hQPt0R"},"source":["# The following code will generate the example. \n","#If you execute this notebook, please make sure you run this cell before trying to run the rest of the code\n","import numpy as np\n","import pandas as pd\n","from sklearn import linear_model as lm\n","import scipy as sp\n","np.random.seed(11)\n","# We will build a simple linear regression model of the kind: Y = a * X_1 + b * X_2 where X_2 contains missing values\n","a = 3\n","b = 2\n","lambda_1 = 2\n","lambda_2 = 1\n","X_1 = np.random.poisson(lambda_1, 100)\n","X_2 = np.random.poisson(lambda_2, 100).astype('O')\n","Y = a*X_1 + b* X_2 + np.random.normal(0, 1.5, 100)\n","missing = np.random.binomial(1, 0.3, 100).astype('bool')\n","data = pd.DataFrame({\"SpentOnBiscuits\" : Y, \"NChildren\" : X_1, \"NDogs\" : X_2})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:12:56.962668Z","start_time":"2019-03-12T07:12:56.915794Z"},"id":"mKFfg30pPt0p"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:17:09.357826Z","start_time":"2019-03-12T07:17:09.279703Z"},"id":"fuUC0_0sPt0_"},"source":["Y = data.SpentOnBiscuits\n","X = data.loc[:,['NChildren', 'NDogs']]\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\"+\n","     \"b_hat = \" +str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJAjqavzPt2S"},"source":["But there is a catch: Several people don't want to report the number of dogs they have to avoid attracting thiefs that would steal their canine companions (a similar problem is not observed for children, and those are reported faithfully).\n","\n","Around 30% of the population is suspicious and do not want to disclose the number of dogs they have. We have to form a prediction nonetheless."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:15:38.698956Z","start_time":"2019-03-12T07:15:38.667708Z"},"id":"vqqYr-JYPt1T"},"source":["missing  # we will erase some data that is missing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:16:28.986136Z","start_time":"2019-03-12T07:16:28.955029Z"},"id":"QdxmOC32Pt1o"},"source":["X_2[missing] = np.NaN\n","data.NDogs = X_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:16:39.573648Z","start_time":"2019-03-12T07:16:39.542410Z"},"id":"CuHeAh55Pt14"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPljPWadPt2X"},"source":["## Option 1: Deleting any line with missing value "]},{"cell_type":"markdown","metadata":{"id":"kBG_r6cTPt2b"},"source":["This is the radical option! You can delete every line where there is a missing value. It will usually work well when you have few missing values. The good thing is that you don't have to guess what the missing values would have been. The bad thing is that you \"lose\" information that may have been useful for estimating the other coefficients as well (be weary though, keeping them may introduce bias in your results)."]},{"cell_type":"markdown","metadata":{"id":"OnRHPFNAPt2f"},"source":["Let's do this! It is done very easily using the function *dropna()*. To use this, you can do it this way:\n","\n","data = data.dropna()\n","\n","In this query, you are applying the function *dropna()* coming from the pandas library on the dataframe and assigning this newly created dataframe to the variable called *data*. Note that just invoking *data.dropna()* won't change the dataframe as this functions returns a copy of the dataframe with less rows, it does not modify the existing dataframe (remember your programming class from BA3?).\n","\n","Try it by yourself:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:20:26.556801Z","start_time":"2019-03-12T07:20:26.525553Z"},"id":"12jB9BmYPt2i"},"source":["print(\"Before dropping missing values, we have \" + str(len(data)) + \" rows\")\n","data1 = data.dropna()\n","print(\"After dropping missing values, we have \" + str(len(data1)) + \" rows\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDiIsm8HPt20"},"source":["We can then use the dataset without missing values to perform the analysis (don't worry if you don't remember how a regression works, we'll see that in a later session)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:20:57.849363Z","start_time":"2019-03-12T07:20:57.833738Z"},"id":"LJwYXtLcPt26"},"source":["Y = data1.SpentOnBiscuits\n","X = data1.loc[:,['NChildren', 'NDogs']]\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\"+\n","     \"b_hat = \" +str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDMuW2RXPt3H"},"source":["Note that those results are pretty similar to the one we find on the original dataset. In this case, dropping the missing values seems to be a good approach."]},{"cell_type":"markdown","metadata":{"id":"4ZaYsjLZPt3M"},"source":["## Option 2: Try to approximate the missing values"]},{"cell_type":"markdown","metadata":{"id":"n3WlNxwQPt3O"},"source":["A somewhat less radical but equally sensitive approach is to fill in missing values with other values. Typical choices are:\n","* Mean\n","* Median\n","* Mode\n","* Min or max\n","* Interpolation"]},{"cell_type":"markdown","metadata":{"id":"3ZeLX-1lPt3S"},"source":["The choice of which statistics or value you'll chose depends on the context and always warrant testing (it is usually easy to test several solutions for the same problem)."]},{"cell_type":"markdown","metadata":{"id":"j_FLhOOvPt3V"},"source":["The most frequent choices are the mean (for continuous variables) or mode (for discrete variables). Fortunately, substituting the missing values by whatever you want is relatively easy.\n","\n","With pandas, you can use the method *fillna()* to do just that."]},{"cell_type":"markdown","metadata":{"id":"hVh9CK4TPt3Y"},"source":["Here is [the link to the documentation for the fillna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)"]},{"cell_type":"markdown","metadata":{"id":"DtEd9OQWPt3b"},"source":["Let's do the first one together.\n","\n","We will start by filling in the missing values with the mean of the column *NDogs*."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:27:13.851184Z","start_time":"2019-03-12T07:27:13.819947Z"},"id":"FvQNPRjGPt3f","scrolled":true},"source":["~data.NDogs.isna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:29:57.301506Z","start_time":"2019-03-12T07:29:57.270246Z"},"id":"CIBx5LZsPt3m"},"source":["# Compute the mean of the field and assign it to every missing value\n","mean_dogs = np.mean(data.NDogs[~data.NDogs.isna()])\n","data2 = data.fillna(mean_dogs)\n","data2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:30:30.811846Z","start_time":"2019-03-12T07:30:30.780606Z"},"id":"pDnN8En_Pt3u"},"source":["# Run the regression\n","Y = data2.SpentOnBiscuits\n","X = data2.loc[:,['NChildren', 'NDogs']]\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\"+\n","     \"b_hat = \" +str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uP2ojf4nPt36"},"source":["Your turn: instead of assigning the mean to the missing value, try assigning the mode. Use the function *sp.stats.mode(x)\\[0\\]\\[0\\]* to compute it. Just replace the X with the field of which you'd like to retrieve the mode.\n","\n","Once this is done, run the next cell and find the estimated coefficients. Is using the mode more accurate than using the mean?"]},{"cell_type":"markdown","metadata":{"id":"ZqDM9Bi2Pt3_"},"source":["Here is the [link to the documentation of scipy.stats.mode()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html) for reference."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:42:47.108203Z","start_time":"2019-03-12T07:42:47.076946Z"},"id":"SCTWNmt7Pt4E"},"source":["# Compute the mode of the field and assign it to every missing value\n","mode_dogs = \n","data3 = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:42:58.392554Z","start_time":"2019-03-12T07:42:58.376945Z"},"id":"14GGye_tPt4R"},"source":["data3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:43:07.619719Z","start_time":"2019-03-12T07:43:07.572813Z"},"id":"mUWu9ubhPt4g"},"source":["# Run the regression\n","Y = data3.SpentOnBiscuits\n","X = data3.loc[:,['NChildren', 'NDogs']]\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\"+\n","     \"b_hat = \" +str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7dd06XgPt4u"},"source":["### Interpolation"]},{"cell_type":"markdown","metadata":{"id":"1WuW_q6JPt40"},"source":["Sometimes, the sequence of the observations has a meaning. Think about sequential observations. If one of the independent variable you are observing are samples coming from a continuous process and the frequency of observation is high enough, it is likely that a missing observation finds itself between two sequential observations."]},{"cell_type":"markdown","metadata":{"id":"u6Plo-47Pt43"},"source":["Imagine you're observing the outside temperature every hour and use it to estimate the number of soda cans a vending machine is selling. The issue is that sometimes, the thermometer sensor has a failure and fails to transmit the temperature reading sometimes.\n","\n","You may imagine that, for lack of a better model, each time you have a \"hole\" in the data, the temperature is between the previous one and the next one. Usually, a linear approximation is \"good enough\" for most purposes."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:46:45.687455Z","start_time":"2019-03-12T07:46:45.625107Z"},"id":"AAeZRb-TPt45"},"source":["# Let's build a pet example once again. The temperature is represented as the product of two sinusoidal function\n","np.random.seed(11)\n","stops = np.arange(0, 80, 0.8)\n","a = 8\n","b = 5\n","X_1 = np.random.normal(14, 2, 100)\n","X_2 = 10 * np.sin(stops) * np.sin(stops + np.random.randn(100)/4)\n","Y = a * X_1 + b * X_2 + np.random.normal(0,0.5,100)\n","missing = np.random.binomial(1, 0.1, 100).astype('bool')\n","missing[0] = False # Otherwise, we can't interpolate the first value\n","missing[-1] = False # same\n","soda = pd.DataFrame({\"sales\" : Y, \"demand\" : X_1, \"temperature\" : X_2})\n","regr = lm.LinearRegression()\n","res = regr.fit(soda.loc[:,[\"demand\", \"temperature\"]], soda.sales)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\" +\n","     \"b_hat = \" + str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:47:54.701840Z","start_time":"2019-03-12T07:47:54.422962Z"},"id":"IdRjhNCoPt5F"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.plot(X_2, marker='o')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:48:46.749097Z","start_time":"2019-03-12T07:48:46.733460Z"},"id":"dgetkeIGPt5Q"},"source":["X_2[missing] = np.NaN\n","soda.temperature = X_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:48:58.422964Z","start_time":"2019-03-12T07:48:58.391708Z"},"id":"GhUF1l3YPt5Z"},"source":["soda.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PNMaRe9Pt5k"},"source":["We'll only see the linear interpolation. but there are other type. Doing it with Pandas is, once again, fairly easy. You can use the *interpolate()* function in pandas to do that in one instruction. Let's do this one together.\n","\n","But before that, try practicing and remove the missing values to see what the result of the regression would be without the missing value rows."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:50:44.917876Z","start_time":"2019-03-12T07:50:44.871176Z"},"id":"g77GgGkZPt5n"},"source":["soda2 = soda.dropna()\n","regr = lm.LinearRegression()\n","res = regr.fit(soda2.loc[:,[\"demand\", \"temperature\"]], soda2.sales)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\" +\n","     \"b_hat = \" + str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHfaLt2JPt6H"},"source":["Let's do this! [The documentation for the interpolation is under this link](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:52:12.437994Z","start_time":"2019-03-12T07:52:12.406739Z"},"id":"DFgaa71dPt6M"},"source":["temp = # extract the temperature column\n","soda3 = # copy the dataframe\n","soda3.temperature = # use interpolation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWg0plk3R0Y3"},"source":["regr = lm.LinearRegression()\n","res = regr.fit(soda3.loc[:,[\"demand\", \"temperature\"]], soda3.sales)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\" +\n","     \"b_hat = \" + str(res.coef_[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:52:23.675704Z","start_time":"2019-03-12T07:52:23.644465Z"},"id":"o0YyWTRgPt6b"},"source":["soda3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6lJpEY8GPt6n"},"source":["# Outliers "]},{"cell_type":"markdown","metadata":{"id":"9MnDOjlPPt6p"},"source":["Missing values are not the only issue you'll face when using data. Outliers are an important topic."]},{"cell_type":"markdown","metadata":{"id":"0frp7CYRPt6r"},"source":["## What is an outlier? "]},{"cell_type":"markdown","metadata":{"id":"R7AXdYTRPt6t"},"source":["The issue is there is no simple or even single definition. Intuitively , it is a value that should not have this value.\n","\n","That's vague, but it is better to think of examples."]},{"cell_type":"markdown","metadata":{"id":"x7g4tZ_JPt6w"},"source":["Some outliers can come from\n","* Measurement errors\n","* Data handling errors\n","* Abnormal situation during the observation (with a caveat)\n","* etc."]},{"cell_type":"markdown","metadata":{"id":"zFbr90sdPt6y"},"source":["Many time, outliers are not too worrisome. However, things can become messy very fast. Consider the following example: We want to find a correlation between height (in meters) and weight (in kg) for people with a regular BMI (neither very over or underweight).\n","\n","This relation is approximately linear (with an error). The issue is that, for some observation, the person in charge of collecting the data recorded the height in centimeters."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:55:52.762863Z","start_time":"2019-03-12T07:55:52.731655Z"},"id":"U4kTsTirPt61"},"source":["# The following code will generate the example. \n","#If you execute this notebook, please make sure you run this cell before trying to run the rest of the code\n","np.random.seed(11)\n","# We will build a simple linear model of weight as a function of the height. The issue is that the 7th observation was recorded in centimers\n","a = 40\n","height = 1.7 + np.random.normal(0, 0.15, 50)\n","weight = a* height + np.random.normal(0, 10, 50)\n","height[6] = height[6]*100\n","data = pd.DataFrame({\"Height\" : height, \"Weight\" : weight})\n","data['sex'] = \"M\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:56:04.606739Z","start_time":"2019-03-12T07:56:04.575492Z"},"id":"Ln3gAbYJPt6-"},"source":["data.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:56:26.470473Z","start_time":"2019-03-12T07:56:26.439265Z"},"id":"CfKR4jkuPt7J"},"source":["# Run the regression\n","Y = data.Weight\n","X = data.Height\n","X = np.asarray(X).reshape(-1,1)\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J7KV8EkAPt7Q"},"source":["That's pretty bad..."]},{"cell_type":"markdown","metadata":{"id":"2ot_-g1yPt7S"},"source":["We can confirm the result we would have if we had the correct value."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:57:34.638254Z","start_time":"2019-03-12T07:57:34.607006Z"},"id":"xrJQjAeRPt7U","scrolled":true},"source":["# Run the regression\n","Y = data.Weight\n","height2 = data.Height.copy()\n","height2[6] = height2[6]/100\n","X = height2\n","X = np.asarray(X).reshape(-1,1)\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqMlQcbePt7f"},"source":["Surely we can do something to avoid the problem? Well yes, but before we do, we need to be very confident that what we observe is indeed an error and, if possible, try to fix it. Alternatively, if we observe something that is so unusual that you don't believe it useful to consider it in your model, you may want to take it out or to \"tune it down\".\n","\n","However, your model will be blind to that kind of occurence. Instead, you may want to use more advanced methods to consider this alternative method in your prediction (for example, you can use what is called Hidden Markov Models, that we will not see in this course)."]},{"cell_type":"markdown","metadata":{"id":"XAFqAvJHPt7h"},"source":["## How to identify outliers?"]},{"cell_type":"markdown","metadata":{"id":"-cZRP0baPt7k"},"source":["Sometimes, you just don't have time to check each entry of the data to determine if it is an outlier. There are some techniques that allow you to narrow the scope of the search."]},{"cell_type":"markdown","metadata":{"id":"snL7PQPePt7m"},"source":["### The graphical one"]},{"cell_type":"markdown","metadata":{"id":"qKISA8HKPt7p"},"source":["Sometimes, you can look at the histogram of the features through a series of boxplot (remember the session about visualization?) to identify outliers."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:58:05.968892Z","start_time":"2019-03-12T07:58:05.796933Z"},"id":"5RgOOHK7Pt7r"},"source":["import matplotlib.pyplot as plt\n","plt.boxplot(data.Height);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qqAgaYUPt70"},"source":["Clearly, we see that one of the value (the round at the top of the plot) is very different from the other (much higher than the 75-percentile)."]},{"cell_type":"markdown","metadata":{"id":"SrjCqnfWPt72"},"source":["The issue is that we don't know which one observation it is. And we need this info to treat it."]},{"cell_type":"markdown","metadata":{"id":"oWogTkLDPt74"},"source":["### Statistics-based methods"]},{"cell_type":"markdown","source":["Only numeric columns can be considered for statistics-based methods."],"metadata":{"id":"w3_mhp64ahon"}},{"cell_type":"code","source":["# Keep only columns that are numeric\n","data = data.select_dtypes(include=['number'])\n","data.head()"],"metadata":{"id":"-LbJs96_acHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A first method could be to look at the different values of the column Height."],"metadata":{"id":"LGTcEQ6AaqSS"}},{"cell_type":"code","source":["# Divide the Height into Bins to spot more easily the outliers\n","data['Height'].value_counts().sort_index()"],"metadata":{"id":"6B2h1EHGa1yI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHhzwrQiPt77"},"source":["A second method might be to look individually at the *n* maximum and minimum values (you'll have to select *n*). This is fairly easy to do with pandas and numpy. Just use *argpartition()*, a function that retrieve the index of minimums (but with negative arguments, you can retrieve the maximums)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:58:52.699563Z","start_time":"2019-03-12T07:58:52.668315Z"},"id":"_2ZSiEfrPt79"},"source":["arr = np.asarray(data.Height)\n","ind_max = np.argpartition(arr, -3)[-3:] # We select the 3 largest values\n","print(\"Indices: \" + str(ind_max) + \"\\n\")\n","print(\"Values: \" + str(arr[ind_max]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T07:58:53.685029Z","start_time":"2019-03-12T07:58:53.653981Z"},"id":"TFzMwstbPt8G"},"source":["arr = np.asarray(data.Height)\n","ind_min = np.argpartition(arr, 3)[:3] # We select the 3 smallest values\n","print(\"Indices: \" + str(ind_min) + \"\\n\")\n","print(\"Values: \" + str(arr[ind_min]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJh1FA4NPt8O"},"source":["But this is unsatisfactory: you still have to do a lot of manual work: If all the values you find are outliers, you need to check if the next one is an outlier too.\n","\n","You can use a more \"automatic\" way. However, it is predicated on strong assumption on your features: namely that they are distributed normally."]},{"cell_type":"markdown","metadata":{"id":"lTwSj6cnPt8Q"},"source":["The idea here is to compute the mean and standard deviation of the sample and to analyze everything that falls outside $[\\bar{x} - 2 \\hat{\\sigma}; \\bar{x} + 2 \\hat{\\sigma}]$. Note, however, that both $\\bar{x}$ and $\\hat{\\sigma}$"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:00:18.969210Z","start_time":"2019-03-12T08:00:18.953592Z"},"id":"x3QSrM34Pt8S"},"source":["x_bar = np.mean(data.Height)\n","sigma_hat = np.std(data.Height)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:00:32.936768Z","start_time":"2019-03-12T08:00:32.921140Z"},"id":"GMNMFj-BPt8e"},"source":["x_bar, sigma_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:01:24.615449Z","start_time":"2019-03-12T08:01:24.584208Z"},"id":"ItowmhmaPt8r"},"source":["outlier = (data.Height < x_bar - 2 * sigma_hat) | (data.Height > x_bar + 2 * sigma_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:01:33.234993Z","start_time":"2019-03-12T08:01:33.203742Z"},"id":"ulcFV9rGPt8y","scrolled":true},"source":["outlier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:01:42.130489Z","start_time":"2019-03-12T08:01:42.099046Z"},"id":"ZlatSUkZPt89"},"source":["data.loc[outlier]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xDZGyGZPt9F"},"source":["As you can see, in this case, the procedure identified the outlier only, but be weary of the skew that outliers can induce on the mean and observed standard deviation lest you'll lose some good quality data."]},{"cell_type":"markdown","source":["We can define a function so that we can more easily reuse the code created. "],"metadata":{"id":"uuyElysFcyHA"}},{"cell_type":"code","source":["# Define a function to find outliers in a numeric column of a data frame\n","def find_outliers(df, col, print_outputs = True):\n","    x_bar = np.mean(df[col])\n","    sigma_hat = np.std(df[col])\n","    outlier = (df[col] < x_bar - 2 * sigma_hat) | (df[col] > x_bar + 2 * sigma_hat)\n","\n","    if print_outputs:\n","        if outlier.sum() > 0:\n","            print(f'For column {col} we have {outlier.sum()} outliers: \\n{df[outlier][col]}')\n","    else:\n","        return outlier"],"metadata":{"id":"hLbMQJIfaJwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_outliers(data, 'Height')"],"metadata":{"id":"tc1FlT55aUQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrXVemoePt9H"},"source":["## How to treat the outliers?"]},{"cell_type":"markdown","metadata":{"id":"8GHuWvtNPt9I"},"source":["Once again, there is not one single solution: you'll probably have to experiment. There are two main class of ways to deal with outliers:\n","* delete them\n","* bring them to values that are not outliers"]},{"cell_type":"markdown","metadata":{"id":"-sS1UkA6Pt9L"},"source":["The first way is radical in some ways and may not be practical if your dataset is small or if you have many features (why throw away many useful value because one field is corrupted?)."]},{"cell_type":"markdown","metadata":{"id":"k7NDpJeKPt9O"},"source":["We can negate the output of the proc√©dure we've done at the last step: *~outlier* will have the value true for any non-outlier and false for the outlier. You can then take the indexed dataset to keep only the non-outliers."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:03:09.218705Z","start_time":"2019-03-12T08:03:09.203082Z"},"id":"4vCVxkR2Pt9S"},"source":["non_outliers = ~outlier\n","print(\"Before dropping the outliers, the dataset has \" + str(len(data)) + \" rows.\")\n","new_data = data.loc[non_outliers].copy()\n","print(\"After dropping outliers, there remains \" + str(len(new_data)) + \" rows.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjBlGvDUPt9c"},"source":["Good!"]},{"cell_type":"markdown","metadata":{"id":"8QLuPnL3Pt9e"},"source":["We could also decide to bring the rows outside the two \"bands\" defined by $\\bar{x} - 2 \\hat{\\sigma}$ and $\\bar{x} + 2 \\hat{\\sigma}$ by setting values outside thoses limits to those limits. This is done relatively easily as such:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:04:32.661757Z","start_time":"2019-03-12T08:04:32.630538Z"},"id":"Rcl8MH7JPt9h"},"source":["min_band = np.mean(data.Height) - 2 * np.std(data.Height)\n","max_band = np.mean(data.Height) + 2 * np.std(data.Height)\n","data.loc[data.Height < min_band, \"Height\"] = min_band\n","data.loc[data.Height > max_band, \"Height\"] = max_band"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:04:33.427390Z","start_time":"2019-03-12T08:04:33.411799Z"},"id":"HqADu-aEPt9p"},"source":["data.loc[6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfEP3RqhPt91"},"source":["As you can see, this is not ideal: the person we registered as being 160 m tall is now \"only\" 49 m. It improved the matter, but still..."]},{"cell_type":"markdown","metadata":{"id":"NSBdIcgQPt95"},"source":["To solve this issue, you could, for example, set the value of the values outside the band to the higher (or smaller) value inside the band. This can be done by combining the two approaches above and left as an exercise (basically: drop the outliers, take the maximum and the minimum values of the feature, store there somewhere and then assign them to the rows outside the bands instead of assigning min_band and max_band)."]},{"cell_type":"markdown","metadata":{"id":"GDdyC-xqPt9-"},"source":["# Duplicates "]},{"cell_type":"markdown","metadata":{"id":"h4mZR62CPt-C"},"source":["The topic of duplicate is somewhat easier to manage.\n","\n","The question is why do you have duplicates in your data? Are they legitimate?"]},{"cell_type":"markdown","metadata":{"id":"qKo6BBtYPt-F"},"source":["Everything depends on how you, or the person who retrieved the data, worked: Remember the class on SQL? Well, if your dataset has a primary key or something like it, you may be confident that duplicates are not wanted and you may get rid of them promptly."]},{"cell_type":"markdown","metadata":{"id":"OrcdFa-8Pt-I"},"source":["On the other hand, if you're not too sure whether or not the duplicates ought to be there, you can always perform sensitivity analysis: Once you decide whether or not to keep it, do your analysis. Once it's done, apply the same pipeline on the data by adding or substracting the data (depending on what you had chosen). If the conclusions or performance of the models remain the same, no issue. If not, you really need to dig deeper to understand whether or not duplicates are legitimate."]},{"cell_type":"markdown","metadata":{"id":"az9fQ-JJPt-K"},"source":["## How to get rid of duplicates? "]},{"cell_type":"markdown","metadata":{"id":"CP1fe68KPt-P"},"source":["The good news is that getting rid of duplicates is easy! You can use pandas' *drop_duplicates* function and be done with it. Let's work a little example."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:08:46.649988Z","start_time":"2019-03-12T08:08:46.618740Z"},"id":"aooIvERBPt-S"},"source":["# The following code will generate the example. \n","#If you execute this notebook, please make sure you run this cell before trying to run the rest of the code\n","np.random.seed(11)\n","# We will build a simple linear model of weight as a function of the height. The issue is that the 7th observation was recorded in centimers\n","a = 40\n","height = 1.7 + np.random.normal(0, 0.15, 500)\n","errors = np.random.normal(0, 10, 500)\n","weight = a* height + errors\n","data = pd.DataFrame({\"Height\" : height, \"Weight\" : weight})\n","# Because it was in two databases, the data regarding obese people (the ones for which the error is the highest) \n","# was imported twice:\n","heavy = data.loc[errors >= np.quantile(errors, 0.75)]\n","data = data.append(heavy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:08:53.152224Z","start_time":"2019-03-12T08:08:53.120987Z"},"id":"JD-6c93TPt-j"},"source":["Y = data.Weight\n","X = data.Height\n","X = np.asarray(X).reshape(-1,1)\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G941JIfJPt-s"},"source":["We can drop the duplicates and run it again. We can do this in SQL or in pandas:\n","\n","First SQL: (you will be using pandasql for assignement 1)"]},{"cell_type":"code","metadata":{"id":"x5cumtkfWeiI"},"source":["!pip install -U pandasql"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1joAhNNmXvCw"},"source":["from pandasql import sqldf\n","pysql = lambda q: sqldf(q, globals()) # define function to execute sql on dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3aaNfB-YDD7"},"source":["datasql = pysql(''' select distinct Height, Weight\n","              from data\n","              ''') # execute your SQL query, it'll return a pandas dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVjEbNB9YOe1"},"source":["Y = datasql.Weight\n","X = datasql.Height\n","X = np.asarray(X).reshape(-1,1)\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j8y0RrbzYTPM"},"source":["Or with pandas"]},{"cell_type":"code","metadata":{"id":"BDXgzgkRWgtf"},"source":["data = data.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:09:33.051185Z","start_time":"2019-03-12T08:09:33.004308Z"},"id":"UFXDBMBJPt-w"},"source":["Y = data.Weight\n","X = data.Height\n","X = np.asarray(X).reshape(-1,1)\n","regr = lm.LinearRegression()\n","res = regr.fit(X, Y)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU9dh231Pt-7"},"source":["The gain is small because of the structure of the dataset and the error, but, with particularly noisy datasets and on certain algorithms (that we will see in the next lectures), the impact may be much larger."]},{"cell_type":"markdown","metadata":{"id":"2vFnbpi5Pt-9"},"source":["# Formatting for analysis"]},{"cell_type":"markdown","metadata":{"id":"w63akJy4Pt_A"},"source":["The last topic for today is the broader topic of formats. Different types of algorithms and different problems require different data format. For example, classification problems usually require the target to be a factor or a string. For time seriers, there are specific date formats that must be assigned to the data in order to work."]},{"cell_type":"markdown","metadata":{"id":"oiJVsPYAPt_C"},"source":["## \"Casting\" of a column"]},{"cell_type":"markdown","metadata":{"id":"1hxpSljQPt_F"},"source":["In programming, the action of constraining data from one type to another is usually referred to as \"casting\". For example, it is easy to cast an integer, for example *12* into a string, in this case \"12\". Working the other way around is more tricky."]},{"cell_type":"markdown","metadata":{"id":"PKXo0iTDPt_I"},"source":["In pandas (and with numpy series in general), you can cast by using the method *astype(t)* where *t* is a string representing the type of data you want to convert the series to."]},{"cell_type":"markdown","metadata":{"id":"306Vt7I_Pt_K"},"source":["Imagine you have a column in your dataframe representing whether the person loves hamburger or not. It is currently coded as an integer: 1 means the person likes hamburgers and 0 means she doesn't."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:12:04.473204Z","start_time":"2019-03-12T08:12:04.426507Z"},"id":"PG2qc-fjPt_N","scrolled":true},"source":["\n","loves_hamburgers = pd.DataFrame(np.random.binomial(1, 0.8, 100), columns=[\"loves_burgers\"])\n","loves_hamburgers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nh4_xmLmPt_X"},"source":["Say we are looking to use an algorithms from a package and, upon reading the docs, we realize that the target column must be coded as a boolean (a True or False value). What to do then?\n","\n","Easy, just use the *astype(t)* function mentioned before with the type *t* set to 'b' (which stands for boolean)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:13:07.775497Z","start_time":"2019-03-12T08:13:07.744254Z"},"id":"FMpEx1OOPt_Y","scrolled":true},"source":["loves_hamburgers.loves_burgers = loves_hamburgers.loves_burgers.astype('bool')\n","loves_hamburgers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAGPfvrzPt_d"},"source":["## Working with time series"]},{"cell_type":"markdown","metadata":{"id":"kTja3itLPt_f"},"source":["Another issue is when time is involved. In this case, you can use the *to_datetime()* function to make sure that the series is casted to a date and hours (up to the nanosecond if this info is available in the original data)."]},{"cell_type":"markdown","metadata":{"id":"C_UjGNCRPt_j"},"source":["Some considerations to have with the dates and times:\n","* While the function is \"smart\" enough to recognize most time format from a string, it may be necessary to help it by specifying the \"format\" argument to indicate where in the string are the day and month.\n","* In the original dataset, you'll often have the date formated either as a string or as a integer. The integer usually indicate the number of seconds that have passed since the 1st of January 1970 (known as the UNIX epoch). If you see it in your data, it is not a mistake, just an alternative way to store time."]},{"cell_type":"markdown","metadata":{"id":"CdtWJvz9Pt_l"},"source":["Let's try this: we'll generate a bunch of dates and try to cast them into real datetime data."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:14:45.847465Z","start_time":"2019-03-12T08:14:45.816376Z"},"id":"MtaFq1ZAPt_n"},"source":["days = np.random.randint(1, 28+1, 100)\n","months = np.random.randint(1, 12+1, 100)\n","years = np.random.randint(2000, 2019+1, 100)\n","strings = np.array([str(days[ii])+\"/\"+str(months[ii])+\"/\"+str(years[ii]) for ii in range(0,100)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:14:47.381735Z","start_time":"2019-03-12T08:14:47.366109Z"},"id":"Zr3ydo3iPt_r"},"source":["strings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:15:40.246543Z","start_time":"2019-03-12T08:15:40.199689Z"},"id":"FveHXrQ_Pt_x"},"source":["data_dates = pd.DataFrame()\n","data_dates[\"dates\"] = pd.to_datetime(strings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:15:40.817210Z","start_time":"2019-03-12T08:15:40.785958Z"},"id":"ScdtJlKlPt_3","scrolled":true},"source":["data_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tIA7iQgPt_7"},"source":["But wait! There's an issue: for days smaller than 12, the function erroneously assumes that it is the month. To go around it, just specify that, when in doubt, the day comes first."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:17:31.786556Z","start_time":"2019-03-12T08:17:31.739676Z"},"id":"_t2wzPSwPt_9","scrolled":true},"source":["data_dates.dates = pd.to_datetime(strings, dayfirst= True)\n","data_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8UoyUM2PuAE"},"source":["Sometimes, it is important to retrieve information that derives from the date. For example, it is sometimes important to know which day of the week it was (imagine building a model where we forecast attendance to an amusement park, it is more likely that it will be high on the weekends).\n","\n","More generally, you may want to break a date into its components. If you want to do so, you can use the following functions:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:19:58.439271Z","start_time":"2019-03-12T08:19:58.408085Z"},"id":"EndCxZeXPuAG"},"source":["data_dates['WeekDay'] = data_dates.dates.dt.day_name()\n","data_dates['WeekDayNum'] = data_dates.dates.dt.dayofweek\n","data_dates['Day'] = data_dates.dates.dt.day\n","data_dates['Month'] = data_dates.dates.dt.month\n","data_dates['Year'] = data_dates.dates.dt.year\n","data_dates['Hour'] = data_dates.dates.dt.hour"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-03-12T08:19:59.064789Z","start_time":"2019-03-12T08:19:59.017912Z"},"id":"jZ6Nvpe4PuAL","scrolled":true},"source":["data_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wePGQOzrYbA"},"source":["Sometimes you also have a bit more specific formats, such as for instance only the month and the year, separated with an forward slash as in the example below:"]},{"cell_type":"code","metadata":{"id":"P4TGR92QrErR"},"source":["all_months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n","months = np.random.choice(all_months, 100)\n","years = np.random.randint(2000, 2021, 100)\n","strings = np.array([str(months[ii])+'/'+str(years[ii]) for ii in range(0,100)])\n","strings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTGc-Arkr1OG"},"source":["In those cases you can also specify the format of the dates, you can have a view [here](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes) on what are the format specifications."]},{"cell_type":"code","metadata":{"id":"UcjryJYJrWO-"},"source":["data_dates.dates = pd.to_datetime(strings, format='%b/%Y')\n","data_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YFCrQ8FPuAS"},"source":["# Your turn "]},{"cell_type":"markdown","metadata":{"id":"BtFo1CLZPuAU"},"source":["Your friend with whom you're working on an assignment where you try to use data about age, wealth and whether or not the person drives an expensive car to predict the number of ties the person possess, sent you the dirty dataset present in the file \"dirty_set.csv\". Clean it up and estimate the coefficient of the linear regression.\n","\n","For the NaN's, substitute them with the mean of their series for continuous variable and the mode of the series for the categorical or boolean ones."]},{"cell_type":"code","metadata":{"id":"hYoGPvodTbsD"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksiqLDXAPuAV"},"source":["df = pd.read_csv(\"/gdrive/MyDrive/STATS406/STATS406 TP5 - dirty_set.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRcPzFeKPuAZ"},"source":["# your cleaning code comes here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6TfA64QPuAe"},"source":["from datetime import datetime\n","ages_in_ns = (datetime.today() - df.date_of_birth) # make sure that the formatted column for the date of birth is called \"date_of_birth\"\n","tot_sec_year = 60 * 60 * 24 * 365\n","years = [ (ii.total_seconds()/tot_sec_year) for ii in ages_in_ns]\n","df['age'] = years\n","regr = lm.LinearRegression()\n","res = regr.fit(df.loc[:,[\"wealth\", \"age\", \"owns_expensive_car\"]], df.nb_ties)\n","print(\"Estimated coefficients are\\n\" + \n","      \"a_hat = \" + str(res.coef_[0]) + \"\\n\" +\n","     \"w_hat = \" + str(res.coef_[1]) + \"\\n\" +\n","     \"o_hat = \" + str(res.coef_[2]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z317BxErUVyB"},"source":[],"execution_count":null,"outputs":[]}]}